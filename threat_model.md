## System Overview

This project evaluates the security and safety robustness of a target LLM by running automated, reproducible adversarial test cases against it. Untrusted input enters through user prompts (single-turn and multi-turn conversations) and is transformed by an attack generation layer (mutations, paraphrasing, encoding, multilingual variants) into candidate adversarial prompts. The platform sends these prompts to the model under test (and optionally a defended/mitigated configuration), collects model outputs, and scores them using an automated evaluator that produces structured metrics (e.g., violation rate, severity, persistence across turns). The systemâ€™s core trust boundary is between untrusted user-controlled text and the model/system instructions; the primary assets are policy compliance, instruction hierarchy integrity, and prevention of unsafe or disallowed outputs. Outputs and results are stored as JSON/CSV (and summarized into a report) with deterministic seeds and version-locked dependencies to enable one-command replays and before/after mitigation comparisons.
